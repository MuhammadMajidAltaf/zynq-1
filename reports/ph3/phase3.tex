\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{cite}
%\usepackage[margin=2in]{geometry}
\begin{document}
\centerline{\Large \bf An Investigative Framework for Automatic Hardware Generation}
\medskip
\centerline{\bf Alexander Lown}
\bigskip

\begin{abstract}
  A feasibility study is conducted into automatically generating hardware for a binary program, allowing offloading of program critical regions to an adjacent configurable logic block for a host processor.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
I investigated the ability to automatically speed-up (or reduce power of) parts of existing applications, without a need to write custom offload hardware for each program, or even to re-compile; in a computing system with both conventional CPUs but also some configurable logic (FPGA fabric-like) area, which I will refer to as an \emph{FPGA-soc}.

One such example of a computing system are the Xilinx Zynq series ICs, of which I have been working using a Zynq ZC702\cite{ZC702} development board, which contains a xc7z020 IC at its core. The xc7z020 IC contains a dual core ARM Cortex A9 'processor' (hereafter referred to as the PS) along with full MMU, DDR3 controller and APB peripherals (UART, Ethernet, SD, etc.), as well as a large programmable logic area based off a 28nm HKMG fabrication (hereafter referred to as the PL). A variety of AXI ports (and assosciated control signals such as user configurable clocks and interrupts) enabling communication between the PS and PL at high speed. Refer to figure \ref{fig:inter-block} for a detailed overview of how the PS and PL are interconnected. Whilst this is a quite specific system, there are strong parallels with a more conventional (as of the 2000s-2015s) PC, which could be supplemented with a PL card over PCI-E, in the same way that graphics tasks are offloaded to custom silicon in the form of a dedicated GPU, or more recent processing assistance designs such as the Xeon Phi.

This project uses the High Performance AXI controllers connected to some soft-IP performing DMA to enable the PL to interact directly with user-memory whilst running. Programs are run on top of Linux on the PS, rather than working only with bare metal enabling a significantly increased use-case. A kernel module (XDMA) grants user-space access to this DMA'd memory enabling any program to take advantage of it for offloading.

The project requires the existence of several connected fragments to work: the default HDL implementation, which is extended with HDL generated from analysis of the elf binary run on the PS in userspace (and optionally fed with profiling data to suggest blocks to preferrentially run on the PS, interacting through DMA using the kernel-exposed interface.

The project is designed currently to only work with basic blocks (BBs) which are sequences of instructions without any conditional, branching or jumping behaviour for simplicity. There is no reason it would not be possible to add support for these constructs in further work.

\begin{figure}[p]
  \includegraphics[width=1.4\textwidth,center]{fig/interconnect-block.eps}
  \caption{Xilinx UG585\cite{ug585} Figure 5-1: Interconnect Block Diagram}
  \label{fig:inter-block}
\end{figure}

%------------------------------------------------------------------------------
\section{Background}
  \subsection{Initial State Investigation}

  I investigated the existing behaviour of the Zynq ZC702 development board using a variety of metrics in several situations, which will prove useful for Phases 2 and 3.
  This report will present an overview of the collected data, and will draw some conclusion for baseline behaviour.\\
  I have used 2 different benchmarks - the VTR 'basic' regression tests, and the encoding of a 10 second clip of 1280x720 resolution video (from Sintel) from YUV (y4m variant) to h264 using the x264 project.
  The choice of this second benchmark is related to one of the 'proposed' end-cases for this research project - namely hardware offloading of video encoding without the need for the encoder to be specifically written to target the Zynq PL.\\
  The collected data includes runtime, power, current, voltage and temperature information from the on-board PMICs, as well as system memory usage. The PS supports 3 speed steps (222MHz, 333MHz and 667MHz), but due to limitations of the peripheral clocks, only the latter two steps are valid states for testing.

\subsubsection{VTR}
  \paragraph{Runtime}
    Table \ref{tab:vtr:rt} shows the runtime for the various PS clock speeds.
    \begin{table}[tbp]
      \centering
      \begin{tabular}{l | l}
        Speed/MHz & Runtime/s \\
        \hline
        333 & 645 \\
        667 & 343 \\
      \end{tabular}
      \caption{Runtime vs Speed of PS for VTR}
      \label{tab:vtr:rt}
    \end{table}
    This suggests that halving the clock speed does not quite double the actual runtime. I propose that this is due to a reduction in the number of clocks spent idling on memory accesses, resulting in a more efficent usage of each clock cycle.

  \paragraph{Power}
    Figures \ref{fig:vtr:pow:667} and \ref{fig:vtr:pow:333} shows the 'power' as reported by the onboard PMIC for the VCCPx rails (power the PS) and VCCADJ rail (powers the DDR3 memory ICs and the corresponding IO banks).
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/vtr:pow:667.eps}
      \caption{Power usage of VTR at 667MHz}
      \label{fig:vtr:pow:667}
    \end{figure}
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/vtr:pow:333.eps}
      \caption{Power usage of VTR at 333MHz}
      \label{fig:vtr:pow:333}
    \end{figure}
    There is a clear uptick in VCCPINT usage after the first 4 samples, which is when the test is started. There is a large deviation in VCCPINT draw over the course of the benchmark at 667MHz, but a signifcantly reduced deviation when run at 333MHz - which I propose is due to the fewer stalls of the processor on memory accesses. On the other hand, VCCADJ's power usage at 333MHz is consistently higher, whilst at 667MHz, it peaks briefly but then returns to baseline for a much longer time - a behaviour I suggest follows on from it being able to complete trannsfers in a shorter time at the higher clock rate, enabling it to return the memory bus to idle sooner.

  \paragraph{Memory}
    Figures \ref{fig:vtr:mem:667} and \ref{fig:vtr:mem:333} show system memory usage for active pages, anonymous pages and the total committed address space during the running of the VTR tests.
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/vtr:mem:667.eps}
      \caption{Memory usage of VTR at 667MHz}
      \label{fig:vtr:mem:667}
    \end{figure}
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/vtr:mem:333.eps}
      \caption{Memory usage of VTR at 333MHz}
      \label{fig:vtr:mem:333}
    \end{figure}
    These show the allocated of anonymous pages for VTR (and its assosciated tools) to run in. The profiles are pretty similar during both sets of tests, due to changing the speed having little effect on the amount of memory needed. The major spikes in committed address space correspond directly to sudden decreases in the number of active pages - possibly an effect of the deallocations having occurred but the address space not being marked as uncommitted until after the next set of allocations has occurred.

\subsubsection{x264}
  \paragraph{Runtime}
    Table \ref{tab:x264:rt} shows the runtime for the various PS clock speeds.
    \begin{table}[tbp]
      \centering
      \begin{tabular}{l | l}
        Speed/MHz & Runtime/s \\
        \hline
        333 & 122 \\
        667 & 62 \\
      \end{tabular}
      \caption{Runtime vs Speed of PS for x264 encoding}
      \label{tab:x264:rt}
    \end{table}
    Unlike the VTR benchmarks, the runtime is unaffected by the speed of the processor core, despite making heaving usage of NEON optimizations, the limitation is clearly elsewhere\ldots

  \paragraph{Power}
    Figures \ref{fig:x264:pow:667} and \ref{fig:x264:pow:333} shows the 'power' as reported by the PMIC for the VCCPx rails and VCCADJ rail.
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/x264:pow:667.eps}
      \caption{Power usage of x264 at 667MHz}
      \label{fig:x264:pow:667}
    \end{figure}
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/x264:pow:333.eps}
      \caption{Power usage of x264 at 333MHz}
      \label{fig:x264:pow:333}
    \end{figure}
    At 667MHz, there is an increase of 250mW on VCCPINT for the ~62 seconds, but for 333MHz, there is an increase of only 120mW for the 122 seconds, indicating that running at 333MHz consumes less power in total for the same task.

  \paragraph{Memory}
    Figures \ref{fig:x264:mem:667} and \ref{fig:x264:mem:333} show system memory usage.
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/x264:mem:667.eps}
      \caption{Memory usage of x264 at 667MHz}
      \label{fig:x264:mem:667}
    \end{figure}
    \begin{figure}[p]
      \includegraphics[width=\textwidth]{data/fig/x264:mem:333.eps}
      \caption{Memory usage of x264 at 333MHz}
      \label{fig:x264:mem:333}
    \end{figure}
    These show a fairly constant memory profile, occupying almost half of the total memory to encode 1280x544p footage.

  \paragraph{Profiling}
    The listings \ref{lst:x264:667} and \ref{lst:x264:333} show the gprof data collected from the benchmarked instances of x264.
    \begin{sidewaysfigure}[p]
      \lstinputlisting[firstline=4, lastline=20]{data/x264/vtr-gmount.out-667-profiled}
      \caption{gprof data for x264 at 667MHz}
      \label{lst:x264:667}
    \end{sidewaysfigure}
    \begin{sidewaysfigure}[p]
      \lstinputlisting[firstline=4, lastline=20]{data/x264/vtr-gmount.out-333-profiled}
      \caption{gprof data for x264 at 333MHz}
      \label{lst:x264:333}
    \end{sidewaysfigure}
    At 667MHz, \verb x264_me_search_ref is the main hotspot - due to lots of stalls on memory accesses, which don't stall at 333MHz - due to the 'reduced' latency the PS sees. At 333MHz, \verb x264_pixel_* routines take up the majority of the time, as these routines are performing lots of arithemtic computation on a few pixels. These look like ideal candidates to offload to the PL. A few of the \verb x264_pixel_* routines are shown in listing \ref{lst:x264:ass} to demonstrate.
    \begin{sidewaysfigure}[bp]
      \centering
      \begin{subfigure}[b]{0.45\textwidth}
        \lstinputlisting[firstline=805, lastline=825]{pixel-a.S}
        \caption{The saturation 8x8 calculations using NEON extensions}
      \end{subfigure}
      \qquad
      \begin{subfigure}[b]{0.45\textwidth}
        \lstinputlisting[firstline=401, lastline=417]{mc-a.S}
        \caption{The average (width 16) calculations using NEON extensions}
      \end{subfigure}
      \caption{ARM Assembly for some of the pixel routines from x264}
      \label{lst:x264:ass}
    \end{sidewaysfigure}

    Further investigation of these results showed that they were not as optimal candidaes for offloading as initially suggested, due to the encoding process at the default levels spending too much time in memory accesses, rather than in pure-computational work. As such the profiling data was re-run with higher quality encoding presets of x264. I chose to try use the presets 'slower', 'very slow' and 'placebo'. An overview of the result can be seen in table \ref{tab:x264:prof-extra} and listings \ref{lst:x264:slower}, \ref{lst:x264:very-slow} and \ref{lst:x264:placebo}.

    \begin{sidewaystable}[tbp]
      \centering
      \begin{tabular}{l | p{12cm} | l | l}
        Profile & Equated Flags & Average FPS & Runtime/s \\
        \hline\\
        slower & --b-adapt 2 --direct auto --me umh --partitions all --rc-lookahead 60 --ref 8 --subme 9 --trellis 2  & 0.55 & 190\\
        very-slow & --b-adapt 2 --bframes 8 --direct auto --me umh --merange 24 --partitions all --ref 16 --subme 10 --trellis 2 --rc-lookahead 60 & 0.30  & 347\\
        placebo & --bframes 16 --b-adapt 2 --direct auto --slow-firstpass --no-fast-pskip --me tesa --merange 24 --partitions all --rc-lookahead 60 --ref 16 --subme 11 --trellis 2  & 0.06 & 1733 \\
      \end{tabular}
      \caption{Runtime and Flags for more difficult x264 settings}
      \label{tab:x264:prof-extra}
    \end{sidewaystable}

    \begin{sidewaysfigure}[p]
      \lstinputlisting[firstline=4,lastline=20]{data/x264/x264-slower.gprof}
      \caption{gprof data for x264 using 'slower'}
      \label{lst:x264:slower}
    \end{sidewaysfigure}
    \begin{sidewaysfigure}[p]
      \lstinputlisting[firstline=4,lastline=20]{data/x264/x264-very-slow.gprof}
      \caption{gprof data for x264 using 'very-slow'}
      \label{lst:x264:very-slow}
    \end{sidewaysfigure}
    \begin{sidewaysfigure}[p]
      \lstinputlisting[firstline=4,lastline=20]{data/x264/x264-placebo.gprof}
      \caption{gprof data for x264 using 'placebo'}
      \label{lst:x264:placebo}
    \end{sidewaysfigure}

    These show quite a different picture to the default profile, we can see a large quantity of PS time (and power) being spent in functions such as \verb x264_pixel_ads{14} which are entirely arithmetic, along with the NEON optimized \verb x264_pxel_sad_x3_*_neon functions. This suggests that x264 is definitely still a good use case for which PL-backed offloading could result in increased performance, as opposed to simply power reduction.

%------------------------------------------------------------------------------
\section{Completed Work}
Due to the large area of interaction required to make this project complete, there has been work on programs in Linux user space, Linux kernel space, and HDLs (VHDL, Verilog and IP block design), utilizing a variety of tools and languages.

  I could include large quantities of this as part of the report, but will chose instead to include only small snippets and point the reader to the Github repositories if they are interested in the full code bases, with all the supporting scripts and build systems - though I warn them that there is ~20-30GB of both proprietary and free tools required to recreate results.

  The currently relevant code bases can be found at \url{https://github.com/alown/zynq}, \url{https://github.com/alown/zynq-xdma} and \url{https://github.com/alown/vtr}.

  Some of the main files (the more interesting ones) are described briefly below:

  In the zynq repository:

  \verb elf/analysis/*  is a Ruby script that performs offline arm elf binary analysis involving splitting the application into BBs, then performing analysis looking for BBs with a high density of easily offloadable arithmetic instructions, and high density of NEON instructions. It is also able to emit VHDL for many of ARMs 'data-processing' instructions (including support for shifted register operands), as well as the 4 standard MUL variants. This VHDL acts like a 'fused instruction' for the entirety of the parallelizable section of the BB. The effectiveness of this depends entirely on the data dependencies in the BB under consideration. This VHDL fits into the \verb offload.vhdl  file.

  \verb elf/offload.vhdl  is a simple wrapper entity for the generated VHDL that presents some signals representing the r0-r13 registers.
  \verb elf/offload-top.vhdl  presents an AXI4-S (AXI-4 streaming) interface to unpack/pack the starting/end values for r0-r13 for the offload block, as well as coordinating everything using a simple FSM.

  The Vivado Project \verb tmp/vivado/[\ldots]simple.xpr  holds together all of the generated offloaded code by linking it to the rest of the system using Xilinx DMA IP over the HP0 AXI port described earlier, and shown in figure \ref{fig:inter-block}. A high-level block diagram showing the interactions between the offloaded code (shown as the \verb addtest  IP block), and the rest of the design can be found in figure \ref{fig:vivado-block}.

  \begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{fig/vivado-block.eps}
    \caption{Vivado Block Design}
    \label{fig:vivado-block}
  \end{sidewaysfigure}

  To avoid spending unnecessary time writing and debugging a Linux kernel module to do DMA, I have chosen to extend from an existing project called XDMA, which presents the DMA IP as a block device at \verb /dev/xdma . The XDMA project also came with a simple user-space shared library to make adding support to a new application as easy as linking to the shared library and the transfer function at appropriate times.

  I verified the functionality of the PL DMA IP, and the whole userspace stack by originally connected the design with a 1024 deep FIFO over the AXI4-S interface, which then let me perform simple write/wait/read tests from userspace to verify a lack of corruption and to get an idea of the speed of such transfers.
  A DMA transfer to/from the FIFO of a $32\text{bit} * 1024 = 4096\text{bytes}$ was able to achieve speeds of ~$190\text{MB/s}$ using an in-PL clock of $150\text{MHz}$.

  In the zynq-xdma repository:

  \verb demo/xdma-offload.c  is a small user-space testing program which performs the required packetization of some possible register values, then transfers to the PL to do work, then reads the resulting register values back out of the DMA engine and prints it back to the user for verification.


  Together the analysis script when fed in a objdump disassembly (to avoid the need to internally handle disassembly the ELF binary format, keeping the complexity of the analysis system lower) and a gprof profiling file to enable location of program critical sections for offloading, the analysis system is able to parse in BBs, detect sequences of arithmetic instructions (support for NEON arithmetic instructions is included), choose some important sections, and then transliterate into VHDL for a certain limited susbset of arithmetic instructions. (ADD, SUB, MOV, MUL in register-register modes). This generated code can then be included in the hardware framework and then synthesised for an appropriate target device. The host program can then be 'modified' (currently manually) to do the appropriate DMA transfers to/from the hardware framework. This enables demonstration of offloading of very simple programs to hardware, allowing us to benchwark the situation. Refer to table \ref{table:offload-simple} for results.

  \begin{table}[tbp]
    \centering
    \begin{tabular}{l|l}
      Mode & Runtime/$\mu S$\\
      \hline\\
      Host CPU only & 1 \\
      Offloaded using DMA & 60
    \end{tabular}
    \caption{Offloading results for a fused 10-deep integer mul-add sequence including DMA transfer time (ma.c)}
    \label{table:offload-simple}
  \end{table}

\section{Evalution}
\subsection{Problems Left}
The transliteration generates VHDL, because of personal preference (and the much stricter types make keeping the distinctions between signed and unsigned operations more visible). To enable this work to be joined up with the open-toolchain that was originally intended (a VTR-backed) flow, this would need to be changed to generate Verilog instead due to synthesis limitations.

Support for significantly more instructions in the transliteration phase would make this work much more applicable to a real-world situation. The original intention was to demonstrate offloading of a critical section of x264 video encoder, but support was never completed due to time limitations and the lack of support for loops (see next).

Support for loops (detection was demonstrated in the analysis program, but variant and invariant detection proved problematic without 'running' the code). In the event that this work was to be completed, then the offloading and recovery latency could be 'masked' through a pipelining design. (A hardware framework with pipelining support was completed and verified in simulation (isim), but never in hardware due to hardware limitations). Were the initial constraints relaxed, and this work was instead converted to an LLVM backend, then LLVM IR provides sufficent information to enable detection and transliteration through forced unrolling or pipelining in simplistic designs.

Support for loading from memory and storing to memory, would remove the basic-block limitation that this work has been hindered by - as most real-work programs make extensive use, and even calculation heavy programs such as x264 continue to do large quantities of memory usage during critical loops. The Zynq platform theoretically enables support for this, through the use of the cache-coherency port, whilst the Linux host will take care of any access to pages that are not currently resident in pages if required. This would increase the complexity of the hardware framework fairly significantly however, as it requires either stalling the offload-pipeline or re-implementing a fair amount of any processors load/store coherency system, with forwarding lines to enable performance to be kept at a reasonable level.

  Hardware limitations mostly resolve around the very high latency to offload any program, due to the large quantity of information needed to be transferred, and the relatively small size of the AXI ports available between the system. Even something as simple as a few registers (when 32-bits each) requires a clock per register/register-pair (if using the ports in 64-bit mode). By the time a DMA transfer has been scheduled in the kernel for each direction, and then completed, the latency is sufficently large to be restrictive (on the order of thousands of CPU clocks at 600MHz). Alternative silicon designs with larger CPU-Hardware interfaces (in the same way CPU-Memory interfaces have consistently increased in size) would alleviate these problems.

\section{Scope for Future Works}
\subsection{Other Works}
Existing attempts at offloading programs such as Xilinx Vivado HLS\cite{hls}, and Maxeler\cite{mf-fpga} Java interfaces both present their own limitations in this field.

Vivado HLS requires extra information over a standard C program to enable efficent offloading, and has resulted in an incompatible C-like language, thereby removing the advantages (of relocation) that writing in C offers.

Maxela requires a completely custom Java program to be written tailored for your end use case, that utilises a variety of Maexla specific types and bindings, completely removing the advantage of a high level language, and requiring the programmer to think about when and how they wish to offload specific sections of code.

Other works can be found in the GPU space, with the recent advances in general purpose computing on previously graphics specific devices, with toolchains based around working with CUDA\cite{cuda} or OpenCL. Once again though, these required either writing your own kernel, or accepting a variety of limitations in a higher level language that is converted.

\subsection{Recommendations for future works}
I would recommend that future works based on the same (or very similar) hardware systems to the Zynq XC7Z0* silicon choose to focus on offloading loops using a pipelined hardware framework to hide the latency issues on the system. Memory loads/stores could be handlede through either the CCP, or through careful handling in hardware, and then transferring and batching back onto the CPU.

Ideally future users would have input into the SOC design enabling a higher bandwidth interface to be specified between the logic and the host CPU, enabling latency problems to be removed. If sufficent control over the host CPU core is available, then it would be possible to allow for reserved instruction space to be used to trigger offloading, without needing to schedule for a peripheral transfer from the host OS.

\section{References}
\bibliographystyle{plain}
\bibliography{phase3}

\end{document}
